#!/usr/bin/env python3
"""
PHM-Vibench Config Manager Skill å·¥å…·å‡½æ•°
æä¾›é…ç½®ç®¡ç†çš„è¾…åŠ©åŠŸèƒ½
"""

import yaml
import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class ConfigValidator:
    """é…ç½®éªŒè¯å™¨"""

    def __init__(self, vbench_components: Dict):
        self.vbench_components = vbench_components
        self.validation_rules = self._load_validation_rules()

    def _load_validation_rules(self) -> Dict:
        """åŠ è½½éªŒè¯è§„åˆ™"""
        return {
            'required_fields': [
                "model.name",
                "model.embedding",
                "model.backbone",
                "model.task_head",
                "task.name",
                "task.lr",
                "task.batch_size"
            ],
            'naming_patterns': {
                'embedding': r'^E_\d{2}_',
                'backbone': r'^B_\d{2}_',
                'task_head': r'^H_\d{2}_',
                'model': r'^M_\d{2}_'
            },
            'value_ranges': {
                'lr': (0.00001, 0.1),
                'batch_size': (1, 512),
                'max_epochs': (1, 1000),
                'd_model': (16, 2048),
                'n_layers': (1, 24),
                'n_heads': (1, 32)
            }
        }

    def validate_config(self, config: Dict) -> Dict:
        """å®Œæ•´é…ç½®éªŒè¯"""
        result = {
            'valid': True,
            'errors': [],
            'warnings': [],
            'suggestions': []
        }

        # éªŒè¯å¿…éœ€å­—æ®µ
        missing_fields = self._check_required_fields(config)
        if missing_fields:
            result['errors'].extend([f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}" for field in missing_fields])
            result['valid'] = False

        # éªŒè¯ç»„ä»¶å‘½å
        naming_issues = self._validate_component_naming(config)
        result['warnings'].extend(naming_issues['warnings'])
        result['suggestions'].extend(naming_issues['suggestions'])

        # éªŒè¯æ•°å€¼èŒƒå›´
        value_issues = self._validate_value_ranges(config)
        result['errors'].extend(value_issues['errors'])
        result['warnings'].extend(value_issues['warnings'])

        # éªŒè¯ç»„ä»¶å…¼å®¹æ€§
        compatibility_issues = self._check_component_compatibility(config)
        result['warnings'].extend(compatibility_issues)

        return result

    def _check_required_fields(self, config: Dict) -> List[str]:
        """æ£€æŸ¥å¿…éœ€å­—æ®µ"""
        missing = []
        for field in self.validation_rules['required_fields']:
            if not self._get_nested_value(config, field):
                missing.append(field)
        return missing

    def _validate_component_naming(self, config: Dict) -> Dict:
        """éªŒè¯ç»„ä»¶å‘½å"""
        result = {'warnings': [], 'suggestions': []}
        model_config = config.get('model', {})

        for component_type, pattern in self.validation_rules['naming_patterns'].items():
            component_name = model_config.get(component_type)
            if component_name and not re.match(pattern, component_name):
                result['warnings'].append(
                    f"{component_type} å‘½åä¸ç¬¦åˆVbenchæ ‡å‡†: {component_name}"
                )
                # æä¾›æ ‡å‡†å‘½åå»ºè®®
                standard_name = self._suggest_standard_name(component_type, component_name)
                if standard_name:
                    result['suggestions'].append(
                        f"å»ºè®®: {component_type}: '{component_name}' -> '{standard_name}'"
                    )

        return result

    def _validate_value_ranges(self, config: Dict) -> Dict:
        """éªŒè¯æ•°å€¼èŒƒå›´"""
        result = {'errors': [], 'warnings': []}
        ranges = self.validation_rules['value_ranges']

        task_config = config.get('task', {})
        model_config = config.get('model', {})

        # éªŒè¯å­¦ä¹ ç‡
        if 'lr' in task_config:
            lr = task_config['lr']
            min_lr, max_lr = ranges['lr']
            if not (min_lr <= lr <= max_lr):
                result['errors'].append(f"å­¦ä¹ ç‡è¶…å‡ºèŒƒå›´: {lr} (åº”åœ¨ {min_lr}-{max_lr} ä¹‹é—´)")

        # éªŒè¯æ‰¹é‡å¤§å°
        if 'batch_size' in task_config:
            batch_size = task_config['batch_size']
            min_bs, max_bs = ranges['batch_size']
            if not (min_bs <= batch_size <= max_bs):
                result['warnings'].append(
                    f"æ‰¹é‡å¤§å°å¯èƒ½ä¸åˆç†: {batch_size} (å»ºè®®èŒƒå›´ {min_bs}-{max_bs})"
                )

        # éªŒè¯æ¨¡å‹å‚æ•°
        if 'd_model' in model_config:
            d_model = model_config['d_model']
            min_dim, max_dim = ranges['d_model']
            if not (min_dim <= d_model <= max_dim):
                result['warnings'].append(
                    f"æ¨¡å‹ç»´åº¦å¯èƒ½ä¸åˆç†: {d_model} (å»ºè®®èŒƒå›´ {min_dim}-{max_dim})"
                )

        return result

    def _check_component_compatibility(self, config: Dict) -> List[str]:
        """æ£€æŸ¥ç»„ä»¶å…¼å®¹æ€§"""
        warnings = []
        model_config = config.get('model', {})
        task_config = config.get('task', {})

        model_name = model_config.get('name', '')
        task_name = task_config.get('name', '')

        # æ£€æŸ¥ISFM_Promptå…¼å®¹æ€§
        if 'ISFM_Prompt' in model_name:
            if task_name != 'hse_contrastive':
                warnings.append("ISFM_Promptæ¨¡å‹å»ºè®®é…åˆhse_contrastiveä»»åŠ¡ä½¿ç”¨")

            embedding = model_config.get('embedding', '')
            if 'HSE' not in embedding:
                warnings.append("ISFM_Promptæ¨¡å‹å»ºè®®é…åˆHSEåµŒå…¥å±‚ä½¿ç”¨")

        # æ£€æŸ¥å°æ ·æœ¬å­¦ä¹ å…¼å®¹æ€§
        if 'few_shot' in task_name.lower():
            task_head = model_config.get('task_head', '')
            if 'distance' not in task_head:
                warnings.append("å°æ ·æœ¬å­¦ä¹ å»ºè®®ä½¿ç”¨è·ç¦»åˆ†ç±»å¤´ (H_02_distance_cla)")

        return warnings

    def _suggest_standard_name(self, component_type: str, current_name: str) -> Optional[str]:
        """å»ºè®®æ ‡å‡†ç»„ä»¶åç§°"""
        # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯æ ‡å‡†åç§°
        standard_components = self.vbench_components.get(f"{component_type}s", {}).get("standard", [])
        if current_name in standard_components:
            return current_name

        # å°è¯•ä»æ˜ å°„ä¸­æŸ¥æ‰¾
        legacy_mapping = self.vbench_components.get(f"{component_type}s", {}).get("legacy_mapping", {})
        if current_name in legacy_mapping:
            return legacy_mapping[current_name]

        # å°è¯•æ¨æ–­æ ‡å‡†åç§°
        if component_type == 'embedding':
            if 'HSE' in current_name:
                if 'v2' in current_name or 'Prompt' in current_name:
                    return "E_01_HSE_v2"
                return "E_01_HSE"
        elif component_type == 'backbone':
            backbone_map = {
                'Dlinear': 'B_04_Dlinear',
                'TimesNet': 'B_06_TimesNet',
                'PatchTST': 'B_08_PatchTST',
                'FNO': 'B_09_FNO'
            }
            if current_name in backbone_map:
                return backbone_map[current_name]
        elif component_type == 'task_head':
            if 'distance' in current_name:
                return 'H_02_distance_cla'
            elif 'linear' in current_name.lower():
                return 'H_01_Linear_cla'

        return None

    def _get_nested_value(self, data: Dict, path: str) -> Any:
        """è·å–åµŒå¥—å­—å…¸å€¼"""
        keys = path.split('.')
        current = data
        for key in keys:
            if isinstance(current, dict) and key in current:
                current = current[key]
            else:
                return None
        return current


class ConfigMerger:
    """é…ç½®åˆå¹¶å™¨"""

    @staticmethod
    def merge_configs(base: Dict, override: Dict) -> Dict:
        """é€’å½’åˆå¹¶é…ç½®"""
        result = base.copy()

        for key, value in override.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = ConfigMerger.merge_configs(result[key], value)
            else:
                result[key] = value

        return result

    @staticmethod
    def apply_dot_notation(config: Dict, updates: Dict) -> Dict:
        """åº”ç”¨ç‚¹å·è¡¨ç¤ºæ³•çš„é…ç½®æ›´æ–°"""
        result = config.copy()

        for path, value in updates.items():
            keys = path.split('.')
            current = result

            # å¯¼èˆªåˆ°æœ€åä¸€çº§çš„çˆ¶çº§
            for key in keys[:-1]:
                if key not in current:
                    current[key] = {}
                current = current[key]

            # è®¾ç½®æœ€ç»ˆå€¼
            current[keys[-1]] = value

        return result


class ConfigTemplateGenerator:
    """é…ç½®æ¨¡æ¿ç”Ÿæˆå™¨"""

    def __init__(self, template_dir: str):
        self.template_dir = Path(template_dir)

    def generate_config_from_template(self, template_name: str, **kwargs) -> Dict:
        """ä»æ¨¡æ¿ç”Ÿæˆé…ç½®"""
        template_path = self.template_dir / f"{template_name}.yaml"

        if not template_path.exists():
            raise FileNotFoundError(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_path}")

        with open(template_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        # åº”ç”¨è‡ªå®šä¹‰å‚æ•°
        if kwargs:
            config = ConfigMerger.apply_dot_notation(config, kwargs)

        return config

    def list_available_templates(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨æ¨¡æ¿"""
        if not self.template_dir.exists():
            return []

        templates = []
        for file in self.template_dir.glob("*.yaml"):
            templates.append(file.stem)
        return templates


class DatasetMapper:
    """æ•°æ®é›†æ˜ å°„å™¨"""

    def __init__(self, dataset_mapping: Dict):
        self.dataset_mapping = dataset_mapping
        self.reverse_mapping = {v: k for k, v in dataset_mapping['system_ids'].items()}

    def get_system_id(self, dataset_name: str) -> Optional[int]:
        """æ ¹æ®æ•°æ®é›†åç§°è·å–ç³»ç»ŸID"""
        # ç›´æ¥æŸ¥æ‰¾
        system_ids = self.dataset_mapping.get('system_ids', {})
        for sys_id, name in system_ids.items():
            if name.lower() == dataset_name.lower():
                return int(sys_id)

        return None

    def get_dataset_name(self, system_id: int) -> Optional[str]:
        """æ ¹æ®ç³»ç»ŸIDè·å–æ•°æ®é›†åç§°"""
        return self.dataset_mapping.get('system_ids', {}).get(str(system_id))

    def expand_dataset_names(self, datasets: List[Union[str, int]]) -> List[int]:
        """å°†æ•°æ®é›†åç§°åˆ—è¡¨è½¬æ¢ä¸ºç³»ç»ŸIDåˆ—è¡¨"""
        system_ids = []
        for dataset in datasets:
            if isinstance(dataset, int):
                system_ids.append(dataset)
            elif isinstance(dataset, str):
                sys_id = self.get_system_id(dataset)
                if sys_id:
                    system_ids.append(sys_id)
                else:
                    logger.warning(f"æœªæ‰¾åˆ°æ•°æ®é›†: {dataset}")

        return system_ids


class PerformanceTargetChecker:
    """æ€§èƒ½ç›®æ ‡æ£€æŸ¥å™¨"""

    def __init__(self, targets: Dict):
        self.targets = targets

    def check_performance(self, results: Dict) -> Dict:
        """æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ€§èƒ½ç›®æ ‡"""
        check_result = {
            'all_targets_met': True,
            'target_results': {},
            'summary': ''
        }

        for target_name, target_value in self.targets.items():
            if target_name in results:
                actual_value = results[target_name]
                met = actual_value >= target_value
                check_result['target_results'][target_name] = {
                    'target': target_value,
                    'actual': actual_value,
                    'met': met,
                    'difference': actual_value - target_value
                }

                if not met:
                    check_result['all_targets_met'] = False

        # ç”Ÿæˆæ‘˜è¦
        if check_result['all_targets_met']:
            check_result['summary'] = "ğŸ‰ æ‰€æœ‰æ€§èƒ½ç›®æ ‡éƒ½å·²è¾¾æˆï¼"
        else:
            failed_targets = [name for name, result in check_result['target_results'].items()
                            if not result['met']]
            check_result['summary'] = f"âš ï¸  ä»¥ä¸‹ç›®æ ‡æœªè¾¾æˆ: {', '.join(failed_targets)}"

        return check_result


def generate_config_filename(task_name: str, model_name: str,
                           timestamp_format: str = "%Y%m%d_%H%M%S") -> str:
    """ç”Ÿæˆé…ç½®æ–‡ä»¶å"""
    timestamp = datetime.now().strftime(timestamp_format)

    # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
    task_clean = re.sub(r'[^\w\-_]', '_', task_name.lower())
    model_clean = re.sub(r'[^\w\-_]', '_', model_name.lower())

    return f"{task_clean}_{model_clean}_{timestamp}.yaml"


def backup_config_file(config_path: str, backup_dir: str = "backups") -> str:
    """å¤‡ä»½é…ç½®æ–‡ä»¶"""
    config_path = Path(config_path)
    backup_dir = Path(backup_dir)

    if not config_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

    backup_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_name = f"{config_path.stem}_backup_{timestamp}{config_path.suffix}"
    backup_path = backup_dir / backup_name

    import shutil
    shutil.copy2(config_path, backup_path)

    logger.info(f"é…ç½®æ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_path}")
    return str(backup_path)


def load_config_with_validation(config_path: str, validator: Optional[ConfigValidator] = None) -> Tuple[Dict, Dict]:
    """åŠ è½½é…ç½®æ–‡ä»¶å¹¶éªŒè¯"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    validation_result = {}
    if validator:
        validation_result = validator.validate_config(config)

    return config, validation_result


def save_config_with_validation(config: Dict, output_path: str,
                               validator: Optional[ConfigValidator] = None) -> bool:
    """ä¿å­˜é…ç½®æ–‡ä»¶å‰è¿›è¡ŒéªŒè¯"""
    # éªŒè¯é…ç½®
    if validator:
        validation_result = validator.validate_config(config)
        if not validation_result['valid']:
            logger.error(f"é…ç½®éªŒè¯å¤±è´¥: {validation_result['errors']}")
            return False

        if validation_result['warnings']:
            logger.warning(f"é…ç½®è­¦å‘Š: {validation_result['warnings']}")

    # ä¿å­˜é…ç½®
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True, indent=2)

    logger.info(f"é…ç½®å·²ä¿å­˜åˆ°: {output_path}")
    return True