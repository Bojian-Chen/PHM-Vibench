#!/usr/bin/env python3
"""
æµ‹è¯•æ¨¡å‹å·¥å‚åŠŸèƒ½
éªŒè¯æ¨¡å‹æ„å»ºå’Œå‰å‘ä¼ æ’­æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import os
from datetime import datetime
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '.')

print("=" * 60)
print("ğŸ¤– æ¨¡å‹å·¥å‚æµ‹è¯•")
print("=" * 60)
print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# å¯¼å…¥torch
try:
    import torch
    print(f"ğŸ”¥ PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"ğŸ® CUDAå¯ç”¨: {'æ˜¯' if torch.cuda.is_available() else 'å¦'}")
    if torch.cuda.is_available():
        print(f"ğŸ® GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
        print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f} GB")
except ImportError:
    print("âŒ PyTorchæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…PyTorch")
    sys.exit(1)

try:
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    print(f"\nğŸ“¦ å¯¼å…¥æ¨¡å—...")
    from src.configs import load_config
    from src.model_factory import build_model
    print(f"âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")

    # åŠ è½½é…ç½®
    config_path = sys.argv[1] if len(sys.argv) > 1 else "script/unified_metric/configs/unified_experiments_1epoch.yaml"
    print(f"\nğŸ“– åŠ è½½é…ç½®: {config_path}")
    config = load_config(config_path)
    print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")

    # æ˜¾ç¤ºæ¨¡å‹é…ç½®
    print(f"\nğŸ¤– æ¨¡å‹é…ç½®:")
    print(f"  - æ¨¡å‹åç§°: {config.model.name}")
    print(f"  - æ¨¡å‹ç±»å‹: {config.model.type}")
    print(f"  - åµŒå…¥å±‚: {config.model.embedding}")
    print(f"  - éª¨å¹²ç½‘ç»œ: {config.model.backbone}")
    print(f"  - ä»»åŠ¡å¤´: {config.model.task_head}")
    print(f"  - æ¨¡å‹ç»´åº¦: {config.model.d_model}")
    print(f"  - è¾“å…¥ç»´åº¦: {config.model.input_dim}")
    print(f"  - è¾“å‡ºç»´åº¦: {config.model.output_dim}")

    # å‡†å¤‡å…ƒæ•°æ®
    print(f"\nğŸ“Š å‡†å¤‡å…ƒæ•°æ®...")
    import pandas as pd
    import numpy as np

    # åˆ›å»ºæ¨¡æ‹Ÿçš„metadata dataframeï¼ŒåŒ…å«å¿…éœ€çš„åˆ—
    df = pd.DataFrame({
        'Dataset_id': [1] * 100,  # 100ä¸ªæ ·æœ¬ï¼Œéƒ½å±äºæ•°æ®é›†1 (æ•´æ•°)
        'Label': np.random.randint(0, 10, 100),  # 10ä¸ªç±»åˆ« (æ•´æ•°)
        'Sample_rate': [1000.0] * 100  # é‡‡æ ·ç‡ (æµ®ç‚¹æ•°)
    })

    # åˆ›å»ºå…¼å®¹çš„metadataå¯¹è±¡
    class MockMetadata:
        def __init__(self, dataframe):
            self.df = dataframe

        def __getitem__(self, key):
            # è¿”å›ç¬¬ä¸€è¡Œçš„æ•°æ®ä½œä¸ºæ ·æœ¬ä¿¡æ¯ï¼Œç¡®ä¿Dataset_idæ˜¯æ•´æ•°
            row = self.df.iloc[0].to_dict()
            row['Dataset_id'] = int(row['Dataset_id'])  # å¼ºåˆ¶è½¬æ¢ä¸ºæ•´æ•°
            row['Label'] = int(row['Label'])  # æ ‡ç­¾ä¹Ÿåº”è¯¥æ˜¯æ•´æ•°
            return row

        @property
        def columns(self):
            return self.df.columns.tolist()

        # æ·»åŠ ä¸çœŸå®metadataå…¼å®¹çš„æ–¹æ³•
        def __len__(self):
            return len(self.df)

        def __iter__(self):
            return iter(self.df.values)

    metadata = MockMetadata(df)
    print(f"  - æ¨¡æ‹Ÿæ•°æ®é›†: {len(df)} ä¸ªæ ·æœ¬")
    print(f"  - æ•°æ®é›†ID: {df['Dataset_id'].unique()}")
    print(f"  - æ ‡ç­¾èŒƒå›´: {df['Label'].min()} - {df['Label'].max()}")
    print(f"  - è¾“å…¥ç»´åº¦: {metadata['input_dim']}")
    print(f"  - ç±»åˆ«æ•°: {metadata['num_classes']}")
    print(f"  - åºåˆ—é•¿åº¦: {metadata['sequence_length']}")

    # æ„å»ºæ¨¡å‹
    print(f"\nğŸ—ï¸ æ„å»ºæ¨¡å‹...")
    start_time = time.time()
    model = build_model(config.model, metadata)
    build_time = time.time() - start_time
    print(f"âœ… æ¨¡å‹æ„å»ºæˆåŠŸ (è€—æ—¶: {build_time:.2f}ç§’)")
    print(f"  - æ¨¡å‹ç±»å‹: {type(model).__name__}")

    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"  - æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  - å†»ç»“å‚æ•°: {total_params - trainable_params:,}")

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    print(f"\nğŸ“¦ å‡†å¤‡æµ‹è¯•æ•°æ®...")
    batch_size = 2  # ä½¿ç”¨å°æ‰¹æ¬¡ä»¥èŠ‚çœå†…å­˜
    seq_len = min(1024, config.data.window_size if hasattr(config.data, 'window_size') else 2048)  # ä½¿ç”¨è¾ƒçŸ­åºåˆ—
    input_dim = config.model.input_dim if hasattr(config.model, 'input_dim') else 1

    # åˆ›å»ºè¾“å…¥å¼ é‡
    x = torch.randn(batch_size, input_dim, seq_len)
    print(f"  - è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"  - è¾“å…¥ç±»å‹: {x.dtype}")

    # åˆ›å»ºå…ƒæ•°æ®ï¼ˆHSEæ¨¡å‹éœ€è¦ï¼‰
    batch_metadata = {
        'dataset_id': torch.tensor([1, 2], dtype=torch.long),
        'domain_id': torch.tensor([1, 1], dtype=torch.long),
        'sample_rate': torch.tensor([1024, 2048], dtype=torch.float32)
    }
    print(f"  - å…ƒæ•°æ®é”®: {list(batch_metadata.keys())}")

    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    if torch.cuda.is_available():
        model = model.cuda()
        x = x.cuda()
        batch_metadata = {k: v.cuda() for k, v in batch_metadata.items()}

    # æ‰§è¡Œå‰å‘ä¼ æ’­
    print(f"\nğŸš€ æ‰§è¡Œå‰å‘ä¼ æ’­...")
    start_time = time.time()

    with torch.no_grad():
        # M_02_ISFM_Promptæ¨¡å‹éœ€è¦file_idå‚æ•°æ¥è·å–system_id
        file_id = 1  # æ¨¡æ‹Ÿæ–‡ä»¶IDï¼Œå¯¹åº”Dataset_id = 1
        outputs = model(x, file_id=file_id)

    forward_time = time.time() - start_time
    print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ (è€—æ—¶: {forward_time:.3f}ç§’)")

    # åˆ†æè¾“å‡º
    if isinstance(outputs, tuple):
        print(f"\nğŸ“Š è¾“å‡ºç»“æ„ (tuple):")
        for i, output in enumerate(outputs):
            if output is not None:
                if hasattr(output, 'shape'):
                    print(f"  - è¾“å‡º {i}: å½¢çŠ¶ {output.shape}, ç±»å‹ {output.dtype}")
                else:
                    print(f"  - è¾“å‡º {i}: ç±»å‹ {type(output)}")
            else:
                print(f"  - è¾“å‡º {i}: None")

        # å‡è®¾ç¬¬ä¸€ä¸ªæ˜¯logits
        if len(outputs) > 0 and outputs[0] is not None:
            logits = outputs[0]
            print(f"\nğŸ“ˆ Logitsè¯¦æƒ…:")
            print(f"  - å½¢çŠ¶: {logits.shape}")
            print(f"  - æ•°å€¼èŒƒå›´: [{logits.min().item():.3f}, {logits.max().item():.3f}]")

            # è®¡ç®—é¢„æµ‹ç±»åˆ«
            if len(logits.shape) > 1:
                predictions = torch.argmax(logits, dim=-1)
                print(f"  - é¢„æµ‹ç±»åˆ«: {predictions.tolist()}")

    elif isinstance(outputs, dict):
        print(f"\nğŸ“Š è¾“å‡ºç»“æ„ (dict):")
        for key, value in outputs.items():
            if value is not None and hasattr(value, 'shape'):
                print(f"  - {key}: å½¢çŠ¶ {value.shape}, ç±»å‹ {value.dtype}")
    else:
        if outputs is not None:
            print(f"\nğŸ“Š è¾“å‡ºè¯¦æƒ…:")
            print(f"  - å½¢çŠ¶: {outputs.shape}")
            print(f"  - ç±»å‹: {outputs.dtype}")
            print(f"  - æ•°å€¼èŒƒå›´: [{outputs.min().item():.3f}, {outputs.max().item():.3f}]")

    # å†…å­˜ä½¿ç”¨æƒ…å†µ
    if torch.cuda.is_available():
        print(f"\nğŸ’¾ GPUå†…å­˜ä½¿ç”¨:")
        print(f"  - å·²åˆ†é…: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
        print(f"  - å·²ç¼“å­˜: {torch.cuda.memory_reserved()/1024**3:.2f} GB")

    # æµ‹è¯•æ¢¯åº¦è®¡ç®—ï¼ˆå¯é€‰ï¼‰
    print(f"\nğŸ”„ æµ‹è¯•æ¢¯åº¦è®¡ç®—...")
    model.train()
    x_grad = torch.randn(batch_size, metadata['input_dim'], seq_len, requires_grad=True)
    if torch.cuda.is_available():
        x_grad = x_grad.cuda()
        batch_metadata_grad = {k: v.cuda() for k, v in batch_metadata.items()}

    try:
        outputs_grad = model(x_grad, batch_metadata_grad)
        if isinstance(outputs_grad, tuple):
            loss = outputs_grad[0].mean()
        else:
            loss = outputs_grad.mean()

        loss.backward()
        print(f"âœ… æ¢¯åº¦è®¡ç®—æˆåŠŸ")
        print(f"  - æŸå¤±å€¼: {loss.item():.4f}")

        # æ£€æŸ¥æ¢¯åº¦
        has_grad = False
        for name, param in model.named_parameters():
            if param.grad is not None:
                has_grad = True
                break
        print(f"  - æ¢¯åº¦å­˜åœ¨: {'æ˜¯' if has_grad else 'å¦'}")
    except Exception as e:
        print(f"âš ï¸ æ¢¯åº¦è®¡ç®—å¤±è´¥: {e}")

    print(f"\nâœ… æ¨¡å‹æµ‹è¯•å®Œæˆ - æ‰€æœ‰åŠŸèƒ½æ­£å¸¸!")
    print(f"  - æ¨¡å‹æ„å»º: âœ…")
    print(f"  - å‰å‘ä¼ æ’­: âœ…")
    print(f"  - è¾“å‡ºæ ¼å¼: âœ…")

except ImportError as e:
    print(f"\nâŒ å¯¼å…¥é”™è¯¯: {e}")
    print(f"è¯·ç¡®ä¿:")
    print(f"1. å·²å®‰è£…æ‰€æœ‰ä¾èµ–")
    print(f"2. åœ¨é¡¹ç›®æ ¹ç›®å½•æ‰§è¡Œæ­¤è„šæœ¬")
    print(f"3. æ¨¡å‹æ–‡ä»¶è·¯å¾„æ­£ç¡®")
    sys.exit(1)

except Exception as e:
    print(f"\nâŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
    print(f"\nè¯¦ç»†ä¿¡æ¯:")
    import traceback
    traceback.print_exc()

    # æä¾›æ•…éšœæ’é™¤å»ºè®®
    print(f"\nğŸ”§ æ•…éšœæ’é™¤å»ºè®®:")
    print(f"1. æ£€æŸ¥æ¨¡å‹é…ç½®æ˜¯å¦æ­£ç¡®")
    print(f"2. ç¡®è®¤è¾“å…¥æ•°æ®å½¢çŠ¶æ˜¯å¦åŒ¹é…")
    print(f"3. éªŒè¯å…ƒæ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®")
    print(f"4. å¦‚æœGPUå†…å­˜ä¸è¶³ï¼Œä½¿ç”¨è¾ƒå°çš„æ‰¹æ¬¡å¤§å°")

    sys.exit(1)

print("\n" + "=" * 60)
print("âœ… æ¨¡å‹å·¥å‚æµ‹è¯•å®Œæˆ")
print("=" * 60)