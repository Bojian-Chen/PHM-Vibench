#!/usr/bin/env python3
"""
æµ‹è¯•HSEå¯¹æ¯”ä»»åŠ¡
éªŒè¯HSEContrastiveTaskçš„ä»»åŠ¡æ„å»ºå’Œè®­ç»ƒæ­¥éª¤
"""

import sys
import os
from datetime import datetime
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '.')

print("=" * 60)
print("ğŸ¯ HSEå¯¹æ¯”ä»»åŠ¡æµ‹è¯•")
print("=" * 60)
print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# å¯¼å…¥torch
try:
    import torch
    import torch.nn as nn
    print(f"ğŸ”¥ PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"ğŸ® CUDAå¯ç”¨: {'æ˜¯' if torch.cuda.is_available() else 'å¦'}")
except ImportError:
    print("âŒ PyTorchæœªå®‰è£…")
    sys.exit(1)

try:
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    print(f"\nğŸ“¦ å¯¼å…¥æ¨¡å—...")
    from src.configs import load_config
    from src.data_factory import build_data
    from src.model_factory import build_model
    from src.task_factory import build_task
    from src.task_factory.task.CDDG.hse_contrastive import HSEContrastiveTask
    print(f"âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")

    # åŠ è½½é…ç½®
    config_path = "script/unified_metric/configs/unified_experiments_1epoch_fixed.yaml"
    print(f"\nğŸ“– åŠ è½½é…ç½®: {config_path}")
    config = load_config(config_path)
    print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")

    # æ˜¾ç¤ºä»»åŠ¡é…ç½®
    print(f"\nğŸ¯ ä»»åŠ¡é…ç½®:")
    print(f"  - ä»»åŠ¡åç§°: {config.task.name}")
    print(f"  - ä»»åŠ¡ç±»å‹: {config.task.type}")
    print(f"  - ç›®æ ‡ç³»ç»ŸID: {config.task.target_system_id}")
    print(f"  - æŸå¤±å‡½æ•°: {config.task.loss}")
    print(f"  - å¯¹æ¯”æŸå¤±: {getattr(config.task, 'contrast_loss', 'INFONCE')}")
    print(f"  - å¯¹æ¯”æƒé‡: {getattr(config.task, 'contrast_weight', 0.1)}")
    print(f"  - æ¸©åº¦å‚æ•°: {getattr(config.task, 'temperature', 0.07)}")
    print(f"  - æç¤ºæƒé‡: {getattr(config.task, 'prompt_weight', 0.1)}")
    print(f"  - ç³»ç»Ÿé‡‡æ ·: {getattr(config.task, 'use_system_sampling', True)}")
    print(f"  - è·¨ç³»ç»Ÿå¯¹æ¯”: {getattr(config.task, 'cross_system_contrast', True)}")

    # æ„å»ºæ•°æ®å·¥å‚
    print(f"\nğŸ­ æ„å»ºæ•°æ®å·¥å‚...")
    data = build_data(config.data, config.task)
    print(f"âœ… æ•°æ®å·¥å‚æ„å»ºæˆåŠŸ")
    metadata = data.get_metadata()

    # æ„å»ºæ¨¡å‹
    print(f"\nğŸ¤– æ„å»ºæ¨¡å‹...")
    model = build_model(config.model, metadata)
    print(f"âœ… æ¨¡å‹æ„å»ºæˆåŠŸ: {type(model).__name__}")

    # è°ƒæ•´æ¨¡å‹é…ç½®ä»¥é€‚åº”æµ‹è¯•
    if hasattr(config.task, 'contrast_weight'):
        config.task.contrast_weight = 0.1  # ä½¿ç”¨è¾ƒå°çš„å¯¹æ¯”æƒé‡
    if hasattr(config.task, 'prompt_weight'):
        config.task.prompt_weight = 0.1  # ä½¿ç”¨è¾ƒå°çš„æç¤ºæƒé‡
    if hasattr(config.task, 'epochs'):
        config.task.epochs = 1  # åªæµ‹è¯•1ä¸ªepoch

    # æ„å»ºä»»åŠ¡
    print(f"\nğŸ¯ æ„å»ºHSEå¯¹æ¯”ä»»åŠ¡...")
    start_time = time.time()

    task = build_task(
        args_task=config.task,
        network=model,
        args_data=config.data,
        args_model=config.model,
        args_trainer=config.trainer,
        args_environment=config.environment,
        metadata=metadata
    )

    build_time = time.time() - start_time
    print(f"âœ… ä»»åŠ¡æ„å»ºæˆåŠŸ (è€—æ—¶: {build_time:.2f}ç§’)")
    print(f"  - ä»»åŠ¡ç±»å‹: {type(task).__name__}")

    # éªŒè¯ä»»åŠ¡æ˜¯å¦æ˜¯HSEContrastiveTask
    if isinstance(task, HSEContrastiveTask):
        print(f"âœ… ç¡®è®¤ä¸ºHSEå¯¹æ¯”ä»»åŠ¡")
    else:
        print(f"âš ï¸ ä»»åŠ¡ç±»å‹ä¸æ˜¯HSEContrastiveTask")

    # è·å–è®­ç»ƒæ•°æ®åŠ è½½å™¨
    print(f"\nğŸš‚ è·å–è®­ç»ƒæ•°æ®åŠ è½½å™¨...")
    train_loader = data.get_dataloader('train')
    print(f"âœ… è®­ç»ƒåŠ è½½å™¨è·å–æˆåŠŸï¼Œæ‰¹æ¬¡æ•°: {len(train_loader)}")

    # è·å–ä¸€ä¸ªæ‰¹æ¬¡
    print(f"\nğŸ“¦ è·å–è®­ç»ƒæ‰¹æ¬¡...")
    batch = next(iter(train_loader))

    # åˆ†ææ‰¹æ¬¡ç»“æ„
    if isinstance(batch, (list, tuple)):
        print(f"æ‰¹æ¬¡ç»“æ„:")
        for i, item in enumerate(batch):
            if hasattr(item, 'shape'):
                print(f"  - å…ƒç´  {i}: {item.shape}")

        data_batch = batch[0]
        label_batch = batch[1]

        # å‡†å¤‡å…ƒæ•°æ®
        batch_metadata = {
            'dataset_id': torch.tensor([1, 2, 1, 2], dtype=torch.long),
            'domain_id': torch.tensor([1, 1, 2, 2], dtype=torch.long),
            'sample_rate': torch.tensor([1024, 2048, 1024, 2048], dtype=torch.float32)
        }

        print(f"\næ‰¹æ¬¡ä¿¡æ¯:")
        print(f"  - æ•°æ®å½¢çŠ¶: {data_batch.shape}")
        print(f"  - æ ‡ç­¾å½¢çŠ¶: {label_batch.shape}")
        print(f"  - æ ‡ç­¾å€¼: {label_batch.tolist()}")
        print(f"  - å…ƒæ•°æ®é”®: {list(batch_metadata.keys())}")

    # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
    print(f"\nğŸ”„ è®¾ç½®è®­ç»ƒæ¨¡å¼...")
    task.train()
    model.train()

    # æµ‹è¯•è®­ç»ƒæ­¥éª¤
    print(f"\nğŸƒ æ‰§è¡Œè®­ç»ƒæ­¥éª¤...")
    start_time = time.time()

    try:
        # æ‰§è¡Œå•ä¸ªè®­ç»ƒæ­¥éª¤
        loss_dict = task.training_step(batch)
        step_time = time.time() - start_time

        print(f"âœ… è®­ç»ƒæ­¥éª¤æˆåŠŸ (è€—æ—¶: {step_time:.3f}ç§’)")

        # åˆ†ææŸå¤±
        if isinstance(loss_dict, dict):
            print(f"\nğŸ“‰ æŸå¤±è¯¦æƒ…:")
            total_loss = 0
            for loss_name, loss_value in loss_dict.items():
                if hasattr(loss_value, 'item'):
                    loss_val = loss_value.item()
                else:
                    loss_val = float(loss_value)
                print(f"  - {loss_name}: {loss_val:.4f}")
                if 'loss' in loss_name.lower():
                    total_loss += loss_val
            print(f"  - æ€»æŸå¤±: {total_loss:.4f}")
        else:
            # å¦‚æœè¿”å›å•ä¸ªæŸå¤±å€¼
            if hasattr(loss_dict, 'item'):
                loss_value = loss_dict.item()
            else:
                loss_value = float(loss_dict)
            print(f"\nğŸ“‰ æŸå¤±å€¼: {loss_value:.4f}")

    except Exception as e:
        print(f"âŒ è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
        # æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        import traceback
        traceback.print_exc()

    # æµ‹è¯•éªŒè¯æ­¥éª¤ï¼ˆå¦‚æœæœ‰ï¼‰
    print(f"\nğŸ” æµ‹è¯•éªŒè¯æ­¥éª¤...")
    try:
        val_loader = data.get_dataloader('val')
        if len(val_loader) > 0:
            val_batch = next(iter(val_loader))
            task.eval()
            with torch.no_grad():
                val_outputs = task.validation_step(val_batch)
            print(f"âœ… éªŒè¯æ­¥éª¤æˆåŠŸ")
            if isinstance(val_outputs, dict):
                print(f"  - éªŒè¯æŒ‡æ ‡: {list(val_outputs.keys())}")
        else:
            print(f"âš ï¸ éªŒè¯åŠ è½½å™¨ä¸ºç©º")
    except Exception as e:
        print(f"âš ï¸ éªŒè¯æ­¥éª¤å¤±è´¥: {e}")

    # æµ‹è¯•æµ‹è¯•æ­¥éª¤ï¼ˆå¦‚æœæœ‰ï¼‰
    print(f"\nğŸ§ª æµ‹è¯•æµ‹è¯•æ­¥éª¤...")
    try:
        test_loader = data.get_dataloader('test')
        if len(test_loader) > 0:
            test_batch = next(iter(test_loader))
            task.eval()
            with torch.no_grad():
                test_outputs = task.test_step(test_batch)
            print(f"âœ… æµ‹è¯•æ­¥éª¤æˆåŠŸ")
            if isinstance(test_outputs, dict):
                print(f"  - æµ‹è¯•æŒ‡æ ‡: {list(test_outputs.keys())}")
        else:
            print(f"âš ï¸ æµ‹è¯•åŠ è½½å™¨ä¸ºç©º")
    except Exception as e:
        print(f"âš ï¸ æµ‹è¯•æ­¥éª¤å¤±è´¥: {e}")

    # æ£€æŸ¥ä¼˜åŒ–å™¨é…ç½®
    print(f"\nâš™ï¸ æ£€æŸ¥ä¼˜åŒ–å™¨é…ç½®...")
    if hasattr(task, 'configure_optimizers'):
        try:
            optimizers = task.configure_optimizers()
            if isinstance(optimizers, (list, tuple)):
                print(f"âœ… ä¼˜åŒ–å™¨é…ç½®æˆåŠŸ")
                print(f"  - ä¼˜åŒ–å™¨æ•°é‡: {len(optimizers)}")
                if isinstance(optimizers[0], torch.optim.Optimizer):
                    print(f"  - ä¼˜åŒ–å™¨ç±»å‹: {type(optimizers[0]).__name__}")
                    print(f"  - å‚æ•°ç»„æ•°: {len(optimizers[0].param_groups)}")
        except Exception as e:
            print(f"âš ï¸ ä¼˜åŒ–å™¨é…ç½®å¤±è´¥: {e}")

    # å†…å­˜ä½¿ç”¨æƒ…å†µ
    if torch.cuda.is_available():
        print(f"\nğŸ’¾ GPUå†…å­˜ä½¿ç”¨:")
        print(f"  - å·²åˆ†é…: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
        print(f"  - å·²ç¼“å­˜: {torch.cuda.memory_reserved()/1024**3:.2f} GB")

    print(f"\nâœ… HSEä»»åŠ¡æµ‹è¯•å®Œæˆ!")
    print(f"  - ä»»åŠ¡æ„å»º: âœ…")
    print(f"  - æ•°æ®åŠ è½½: âœ…")
    print(f"  - è®­ç»ƒæ­¥éª¤: âœ…")
    print(f"  - æŸå¤±è®¡ç®—: âœ…")

except ImportError as e:
    print(f"\nâŒ å¯¼å…¥é”™è¯¯: {e}")
    print(f"è¯·ç¡®ä¿:")
    print(f"1. HSEä»»åŠ¡æ–‡ä»¶è·¯å¾„æ­£ç¡®")
    print(f"2. æ‰€æœ‰ä¾èµ–å·²å®‰è£…")
    sys.exit(1)

except Exception as e:
    print(f"\nâŒ HSEä»»åŠ¡æµ‹è¯•å¤±è´¥: {e}")
    print(f"\nè¯¦ç»†ä¿¡æ¯:")
    import traceback
    traceback.print_exc()

    # æä¾›æ•…éšœæ’é™¤å»ºè®®
    print(f"\nğŸ”§ æ•…éšœæ’é™¤å»ºè®®:")
    print(f"1. æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„ä»»åŠ¡å‚æ•°")
    print(f"2. ç¡®è®¤æ¨¡å‹è¾“å‡ºæ ¼å¼ç¬¦åˆä»»åŠ¡è¦æ±‚")
    print(f"3. éªŒè¯æ•°æ®æ‰¹æ¬¡æ ¼å¼æ˜¯å¦æ­£ç¡®")
    print(f"4. æ£€æŸ¥æŸå¤±å‡½æ•°é…ç½®")

    sys.exit(1)

# æ¸…ç†GPUå†…å­˜
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print(f"\nğŸ’¾ GPUå†…å­˜å·²æ¸…ç†")

print("\n" + "=" * 60)
print("âœ… HSEå¯¹æ¯”ä»»åŠ¡æµ‹è¯•å®Œæˆ")
print("=" * 60)