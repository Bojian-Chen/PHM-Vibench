{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "213371a2",
   "metadata": {},
   "source": [
    "# Vbench æ¡†æ¶æµ‹è¯•ç¬”è®°æœ¬-DG pipeline \n",
    "\n",
    "è¿™ä¸ªç¬”è®°æœ¬æä¾›äº†ä¸€å¥—å…¨é¢çš„æµ‹è¯•åŠŸèƒ½ï¼Œç”¨äºéªŒè¯Vbenchæ¡†æ¶çš„å„ä¸ªå­æ¨¡å—æ˜¯å¦èƒ½å¤Ÿæ­£å¸¸å·¥ä½œã€‚é€šè¿‡è¿™ä¸ªç¬”è®°æœ¬ï¼Œæ‚¨å¯ä»¥ï¼š\n",
    "\n",
    "1. æµ‹è¯•æ•°æ®é›†åŠ è½½å’Œå¤„ç†åŠŸèƒ½\n",
    "2. æµ‹è¯•æ¨¡å‹æ„å»ºå’Œå‰å‘ä¼ æ’­\n",
    "3. æµ‹è¯•ä»»åŠ¡å®šä¹‰å’Œæ‰§è¡Œ\n",
    "4. æµ‹è¯•è®­ç»ƒå™¨åŠŸèƒ½\n",
    "5. éªŒè¯å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹\n",
    "6. å¯è§†åŒ–æ¨¡å‹æ€§èƒ½å’Œæ•°æ®åˆ†å¸ƒ\n",
    "\n",
    "è®©æˆ‘ä»¬å¼€å§‹è¿›è¡Œæµ‹è¯•ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a543ffd",
   "metadata": {},
   "source": [
    "### å›¾æ ‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec2536",
   "metadata": {},
   "source": [
    "## æµ‹è¯•ç¯å¢ƒè®¾ç½®\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬å°†è®¾ç½®æµ‹è¯•ç¯å¢ƒï¼ŒåŒ…æ‹¬å¿…è¦çš„ç›®å½•ç»“æ„å’Œé…ç½®æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4e104b",
   "metadata": {},
   "source": [
    "### å·¥ä½œåŒºï¼Œåªè¿è¡Œä¸€æ¬¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "013daf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“å‰ç›®å½•: /home/user/LQ/B_Signal/Signal_foundation_model/Vbench\n",
      "åˆ‡æ¢å·¥ä½œç›®å½•åˆ°é¡¹ç›®æ ¹: /home/user/LQ/B_Signal/Signal_foundation_model/Vbench\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "# è·å–å½“å‰ç›®å½•\n",
    "current_dir = os.getcwd()\n",
    "print(f\"å½“å‰ç›®å½•: {current_dir}\")\n",
    "\n",
    "# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•ä¸ºä¸Šä¸€çº§ç›®å½•\n",
    "\n",
    "if 'project_root' not in globals():\n",
    "    project_root = os.path.dirname(current_dir)\n",
    "    print(f\"è®¾ç½®é¡¹ç›®æ ¹ç›®å½•: {project_root}\")\n",
    "os.chdir(project_root)\n",
    "print(f\"åˆ‡æ¢å·¥ä½œç›®å½•åˆ°é¡¹ç›®æ ¹: {os.getcwd()}\")\n",
    "\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"âœ… å·²å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„: {project_root}\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52acc22",
   "metadata": {},
   "source": [
    "### å¯¼å…¥åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "071426ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®æ¨¡å—ï¼\n",
      "è¯·æ£€æŸ¥é¡¹ç›®ç»“æ„å’Œå®‰è£…ä¾èµ–ã€‚\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.utils.config_utils import load_config, makedir, path_name, transfer_namespace\n",
    "from src.data_factory import build_data\n",
    "from src.model_factory import build_model\n",
    "from src.task_factory import build_task\n",
    "from src.trainer_factory import build_trainer\n",
    "\n",
    "\n",
    "print(\"âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®æ¨¡å—ï¼\")\n",
    "print(\"è¯·æ£€æŸ¥é¡¹ç›®ç»“æ„å’Œå®‰è£…ä¾èµ–ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7dddbe",
   "metadata": {},
   "source": [
    "### å¯¼å…¥é…ç½®æ–‡ä»¶\n",
    "\n",
    "è®°å¾—ä¿®æ”¹ç¯å¢ƒå˜é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f252ba24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] åŠ è½½é…ç½®æ–‡ä»¶: configs/demo/Single_DG/THU.yaml\n",
      "/home/user/LQ/B_Signal/Signal_foundation_model/Vbench\n",
      "[INFO] è®¾ç½®ç¯å¢ƒå˜é‡: WANDB_MODE=disabled\n",
      "[INFO] è®¾ç½®ç¯å¢ƒå˜é‡: VBENCH_HOME=/home/lq/LQcode/2_project/PHMBench/Vbench\n",
      "[INFO] è®¾ç½®ç¯å¢ƒå˜é‡: PYTHONPATH=/home/lq/.conda/envs/lq\n"
     ]
    }
   ],
   "source": [
    "config_path='configs/demo/Single_DG/THU.yaml' \n",
    "\n",
    "print(f\"[INFO] åŠ è½½é…ç½®æ–‡ä»¶: {config_path}\")\n",
    "configs = load_config(config_path)\n",
    "\n",
    "# ç¡®ä¿é…ç½®ä¸­åŒ…å«å¿…è¦çš„éƒ¨åˆ†\n",
    "required_sections = ['data', 'model', 'task', 'trainer', 'environment']\n",
    "for section in required_sections:\n",
    "    if section not in configs:\n",
    "        print(f\"[ERROR] é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ {section} éƒ¨åˆ†\")\n",
    "\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡å’Œå‘½åç©ºé—´\n",
    "args_environment = transfer_namespace(configs.get('environment', {}))\n",
    "\n",
    "args_data = transfer_namespace(configs.get('data', {}))\n",
    "\n",
    "args_model = transfer_namespace(configs.get('model', {}).get('args', {}))\n",
    "args_model.name = configs['model'].get('name', 'default')\n",
    "\n",
    "args_task = transfer_namespace(configs.get('task', {}).get('args', {}))\n",
    "args_task.name = configs['task'].get('name', 'default')\n",
    "\n",
    "args_trainer = transfer_namespace(configs.get('trainer', {}).get('args', {}))\n",
    "args_trainer.name = configs['trainer'].get('name', 'default')\n",
    "\n",
    "for key, value in configs['environment'].items():\n",
    "    if key.isupper():\n",
    "        os.environ[key] = str(value)\n",
    "        print(f\"[INFO] è®¾ç½®ç¯å¢ƒå˜é‡: {key}={value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3258b4cf",
   "metadata": {},
   "source": [
    "### æµ‹è¯•ç›®å½•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "be5fa942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ç›®å½•å·²å‡†å¤‡: /home/user/LQ/B_Signal/Signal_foundation_model/Vbench/results\n",
      "ğŸ“ ç›®å½•å·²å‡†å¤‡: /home/user/LQ/B_Signal/Signal_foundation_model/Vbench/data/processed\n",
      "ğŸ“ ç›®å½•å·²å‡†å¤‡: /home/user/LQ/B_Signal/Signal_foundation_model/Vbench/data/raw\n",
      "ğŸ“ ç›®å½•å·²å‡†å¤‡: /home/user/LQ/B_Signal/Signal_foundation_model/Vbench/save\n",
      "ğŸ“ ç›®å½•å·²å‡†å¤‡: /home/user/LQ/B_Signal/Signal_foundation_model/Vbench/test/results\n",
      "âœ… æµ‹è¯•é…ç½®æ–‡ä»¶å·²å­˜åœ¨: /home/user/LQ/B_Signal/Signal_foundation_model/Vbench/configs/demo/dummy_test.yaml\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºå¿…è¦çš„ç›®å½•\n",
    "test_dirs = [\n",
    "    os.path.join(project_root, \"results\"),\n",
    "    os.path.join(project_root, \"data/processed\"),\n",
    "    os.path.join(project_root, \"data/raw\"),\n",
    "    os.path.join(project_root, \"save\"),\n",
    "    os.path.join(project_root, \"test/results\") \n",
    "]\n",
    "\n",
    "for d in test_dirs:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "    print(f\"ğŸ“ ç›®å½•å·²å‡†å¤‡: {d}\")\n",
    "\n",
    "# è®¾ç½®é»˜è®¤æµ‹è¯•é…ç½®è·¯å¾„\n",
    "default_config_path = os.path.join(project_root, \"configs/demo/dummy_test.yaml\")\n",
    "\n",
    "# æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "if os.path.exists(default_config_path):\n",
    "    print(f\"âœ… æµ‹è¯•é…ç½®æ–‡ä»¶å·²å­˜åœ¨: {default_config_path}\")\n",
    "\n",
    "path, name = path_name(configs, iteration = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa93fe5",
   "metadata": {},
   "source": [
    "## 1. data_factory æ•°æ®å·¥å‚æµ‹è¯•\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29636065",
   "metadata": {},
   "source": [
    "### data_factory æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f211c5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç­›é€‰å‰å…ƒæ•°æ®è¡Œæ•°: 47478\n",
      "ç­›é€‰åå…ƒæ•°æ®è¡Œæ•°: 24\n",
      "æ‰€æœ‰ç›®æ ‡æ•°æ®éƒ½åœ¨ç¼“å­˜ä¸­ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜æ–‡ä»¶: /home/user/data/PHMbenchdata/metadata_5_data.h5\n",
      "Initializing training and validation datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating train/val datasets: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing test datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating test datasets: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®é›†å¤§å°: 46011\n",
      "æ•°æ®é›†å¤§å°: 2949124\n",
      "æ•°æ®åŠ è½½å™¨å¤§å°: 92161\n"
     ]
    }
   ],
   "source": [
    "# ç¬¬ä¸€æ¬¡è¿è¡Œæ„å»ºcacheï¼Œcache æ ¹æ®meta_dataæ–‡ä»¶è¿›è¡Œå‘½å\n",
    "data_factory = build_data(args_data,args_task)\n",
    "# ç¬¬äºŒæ¬¡è¿è¡Œå¯ä»¥ç›´æ¥è¯»å–cache\n",
    "data = data_factory.get_data()\n",
    "print(f\"æ•°æ®é›†å¤§å°: {len(data)}\")\n",
    "dataset = data_factory.get_dataset()\n",
    "print(f\"æ•°æ®é›†å¤§å°: {len(dataset)}\")\n",
    "dataloader = data_factory.get_dataloader()\n",
    "print(f\"æ•°æ®åŠ è½½å™¨å¤§å°: {len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c20f706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®é›†å¤§å°: 46011\n",
      "æ•°æ®é›†å¤§å°: 2949124\n",
      "æ•°æ®åŠ è½½å™¨å¤§å°: 92161\n"
     ]
    }
   ],
   "source": [
    "data = data_factory.get_data()\n",
    "print(f\"æ•°æ®é›†å¤§å°: {len(data)}\")\n",
    "dataset = data_factory.get_dataset()\n",
    "print(f\"æ•°æ®é›†å¤§å°: {len(dataset)}\")\n",
    "dataloader = data_factory.get_dataloader()\n",
    "print(f\"æ•°æ®åŠ è½½å™¨å¤§å°: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fa61fa",
   "metadata": {},
   "source": [
    "### loop dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af39d09d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 129, in collate\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 129, in <dictcomp>\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 121, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 183, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 183, in <listcomp>\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\nValueError: given numpy array has byte order different from the native byte order. Conversion between byte orders is currently not supported.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# åªè¿­ä»£ä¸€ä¸ªæ ·æœ¬è¿›è¡Œæµ‹è¯•\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ((inputs,labels), name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mç¬¬ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m æ‰¹æ•°æ®:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mè¾“å…¥: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 129, in collate\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 129, in <dictcomp>\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 121, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 183, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 183, in <listcomp>\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\nValueError: given numpy array has byte order different from the native byte order. Conversion between byte orders is currently not supported.\n"
     ]
    }
   ],
   "source": [
    "# åªè¿­ä»£ä¸€ä¸ªæ ·æœ¬è¿›è¡Œæµ‹è¯•\n",
    "for i, ((inputs,labels), name) in enumerate(dataloader):\n",
    "    print(f\"ç¬¬ {i+1} æ‰¹æ•°æ®:\")\n",
    "    print(f\"è¾“å…¥: {inputs.shape}\")\n",
    "    # print(f\"è¾“å…¥: {inputs}\")\n",
    "    print(f\"æ ‡ç­¾: {labels}\")\n",
    "    print(f\"åç§°: {name}\")\n",
    "    if i > 1000:\n",
    "        break  # åªå¤„ç†ç¬¬ä¸€ä¸ªæ‰¹æ¬¡\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91223166",
   "metadata": {},
   "source": [
    "## 2. model factory æ¨¡å‹å·¥å‚æµ‹è¯•\n",
    "\n",
    "æµ‹è¯•æ¨¡å‹çš„æ„å»ºå’Œå‰å‘ä¼ æ’­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5b930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸå¯¼å…¥æ¨¡å‹æ¨¡å—: Transformer_Dummy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = build_model(args_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c928f",
   "metadata": {},
   "source": [
    "## 3. task_factory ä»»åŠ¡å·¥å‚æµ‹è¯•\n",
    "\n",
    "æµ‹è¯•ä»»åŠ¡çš„å®šä¹‰å’Œæ‰§è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ef4f643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸå¯¼å…¥æ¨¡å—: src.task_factory.Classification.DG\n",
      "Found 1 unique data IDs\n",
      "Maximum labels per data ID: {'RM_001_CWRU': 3.0}\n",
      "æˆåŠŸå®ä¾‹åŒ–ä»»åŠ¡ç±»: src.task_factory.Classification.DG\n"
     ]
    }
   ],
   "source": [
    "task= build_task(\n",
    "    args_task = args_task,\n",
    "    network = model,\n",
    "    args_data = args_data,\n",
    "    args_model = args_model,\n",
    "    args_trainer = args_trainer,\n",
    "    args_environment = args_environment,\n",
    "    metadata = data_factory.get_metadata()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b77010",
   "metadata": {},
   "source": [
    "## 4. trainer_factory è®­ç»ƒå™¨å·¥å‚æµ‹è¯•\n",
    "\n",
    "æµ‹è¯•è®­ç»ƒå™¨çš„æ„å»ºå’Œç®€å•è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55e4e06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸå¯¼å…¥è®­ç»ƒå™¨æ¨¡å—: src.trainer_factory.Default_trainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸå®ä¾‹åŒ–è®­ç»ƒå™¨: Default_trainer\n"
     ]
    }
   ],
   "source": [
    "trainer = build_trainer(\n",
    "    args_environment,\n",
    "    args_trainer,  # è®­ç»ƒå‚æ•° (Namespace)\n",
    "    args_data,     # æ•°æ®å‚æ•° (Namespace)\n",
    "    path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1193c464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fbdfb5fe0e0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_factory.get_dataloader('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c028d",
   "metadata": {},
   "source": [
    "## 5. pipeline å®Œæ•´æµç¨‹é›†æˆæµ‹è¯•\n",
    "\n",
    "æµ‹è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½åˆ°å®Œæ•´è®­ç»ƒæµç¨‹çš„æ‰€æœ‰ç¯èŠ‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eeb032c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: save/Meta_metadata_5_data.csv/Model_Transformer_Dummy/Task_Classification_Trainer_Default_trainer_20250525_082708/iter_1/logs\n",
      "/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /home/user/LQ/B_Signal/Signal_foundation_model/Vbench/save/Meta_metadata_5_data.csv/Model_Transformer_Dummy/Task_Classification_Trainer_Default_trainer_20250525_082708/iter_1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | network | Model            | 100 K  | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "2 | metrics | ModuleDict       | 0      | train\n",
      "-----------------------------------------------------\n",
      "100 K     Trainable params\n",
      "0         Non-trainable params\n",
      "100 K     Total params\n",
      "0.402     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/126489 [00:00<?, ?it/s]                        "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 144, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 144, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 144, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 144, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 121, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 183, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 121, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 173, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 42\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# TODO load best checkpoint\u001b[39;00m\n\u001b[1;32m     44\u001b[0m task \u001b[38;5;241m=\u001b[39m load_best_model_checkpoint(task,trainer)\n\u001b[1;32m     45\u001b[0m result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest(task,data_factory\u001b[38;5;241m.\u001b[39mget_dataloader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1030\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1030\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:212\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_profiler()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ITERATOR_RETURN:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumed[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 144, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 144, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 144, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 144, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 121, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 183, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 121, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/user/anaconda3/envs/LQ_signal/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 173, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "def load_best_model_checkpoint(model: LightningModule, trainer: Trainer) -> LightningModule:\n",
    "    \"\"\"\n",
    "    åŠ è½½è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜çš„æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "    - model: è¦åŠ è½½æ£€æŸ¥ç‚¹æƒé‡çš„æ¨¡å‹å®ä¾‹ã€‚\n",
    "    - trainer: ç”¨äºè®­ç»ƒæ¨¡å‹çš„è®­ç»ƒå™¨å®ä¾‹ã€‚\n",
    "\n",
    "    è¿”å›:\n",
    "    - åŠ è½½äº†æœ€ä½³æ£€æŸ¥ç‚¹æƒé‡çš„æ¨¡å‹å®ä¾‹ã€‚\n",
    "    \"\"\"\n",
    "    # ä»trainerçš„callbacksä¸­æ‰¾åˆ°ModelCheckpointå®ä¾‹ï¼Œå¹¶è·å–best_model_path\n",
    "    model_checkpoint = None\n",
    "    for callback in trainer.callbacks:\n",
    "        if isinstance(callback, ModelCheckpoint):\n",
    "            model_checkpoint = callback\n",
    "            break\n",
    "\n",
    "    if model_checkpoint is None:\n",
    "        raise ValueError(\"ModelCheckpoint callback not found in trainer's callbacks.\")\n",
    "\n",
    "    best_model_path = model_checkpoint.best_model_path\n",
    "    print(f\"Best model path: {best_model_path}\")\n",
    "\n",
    "    # ç¡®ä¿æœ€ä½³æ¨¡å‹è·¯å¾„ä¸æ˜¯ç©ºçš„\n",
    "    if not best_model_path:\n",
    "        raise ValueError(\"No best model path found. Please check if the training process saved checkpoints.\")\n",
    "\n",
    "    # åŠ è½½æœ€ä½³æ£€æŸ¥ç‚¹\n",
    "\n",
    "    state_dict = torch.load(best_model_path)\n",
    "    model.load_state_dict(state_dict['state_dict'])\n",
    "    return model\n",
    "\n",
    "trainer.fit(task,data_factory.get_dataloader('train'),\n",
    "            data_factory.get_dataloader('val')) # TODO load best checkpoint\n",
    "task = load_best_model_checkpoint(task,trainer)\n",
    "result = trainer.test(task,data_factory.get_dataloader('test'))\n",
    "# ä¿å­˜ç»“æœ\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df.to_csv(os.path.join(path, f'test_result_{1}.csv'), index=False)\n",
    "if args_trainer.wandb:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42487cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LQ_signal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
