# UXFD post-run check + plotting config (standalone; NOT the 5-block vibench config)

input:
  # Option A: auto-discover run dirs by scanning for manifests under a root dir.
  mode: manifest_glob
  # `root_dir` can be an absolute path or a repo-relative path.
  root_dir: save
  # Glob is written in Unix style; on Windows it is still interpreted by Python's Path.glob.
  manifests_glob: "**/artifacts/manifest.json"

  # Option B: explicit run dirs (takes precedence if mode=run_dirs)
  run_dirs: []

checks:
  enable: true
  # If true: a run fails eligibility when any required pattern is missing.
  # If false: missing required patterns are only reported (best-effort).
  fail_on_missing_required: true
  required:
    - "artifacts/manifest.json"
    - "config_snapshot.yaml"
  optional:
    - "logs/**/metrics.csv"
    - "test_result_*.csv"
    - "figures"

plots:
  enable: true
  out_dir: figures
  # Common choices for papers: png + pdf. (eps is also possible but may require extra backend support.)
  save_formats: ["png", "pdf"]
  items:
    - type: learning_curve
      metrics_glob: "logs/**/metrics.csv"
      # If empty, the script will auto-pick common columns like loss/acc.
      series: []
    - type: confusion_matrix
      predictions_glob: "artifacts/predictions.npz"
      normalize: true
